{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec7fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from random import shuffle\n",
    "from perturbation_functions import get_preds_and_scores, calc_suff, calc_necc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b59671b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['orig_texts', 'necc_perturbed', 'suff_perturbed', 'necc_masks', 'suff_masks'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perts = pickle.load(open(\"Data/HateCheck_necc_suff_perturbations_3.pickle\",\"rb\"))\n",
    "perts['orig_texts'] = [tt.strip(' \\n') for tt in perts['orig_texts']]\n",
    "perts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd0780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "necc_masked = []\n",
    "for orig_text, necc_mask in zip(perts['orig_texts'], perts['necc_masks']):\n",
    "    orig_text = orig_text.strip().split()\n",
    "    masked = []\n",
    "    for masks in necc_mask:\n",
    "        masked.append(\" \".join(['[MASK]' if mm else tt for tt, mm in zip(orig_text, masks)]))\n",
    "    necc_masked.append(masked)\n",
    "    \n",
    "suff_masked = [] \n",
    "for orig_text, suff_mask in zip(perts['orig_texts'], perts['suff_masks']):\n",
    "    orig_text = orig_text.strip().split()\n",
    "    masked = []\n",
    "    for masks in suff_mask:\n",
    "        masked.append(\" \".join(['[MASK]' if mm else tt for tt, mm in zip(orig_text, masks)]))\n",
    "    suff_masked.append(masked)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4207f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cd61b077b644a9bbf867a1a56b3ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569ccb54a3334155939fa9e3b4da063f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828866b334f543e18601795e8702cb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fba2bc9e8e446fb895f8a1d0b62b65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# add special tokens for URLs, emojis and mentions (--> see pre-processing)\n",
    "special_tokens_dict = {'additional_special_tokens': ['[USER]','[EMOJI]','[URL]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "datasets = ['CAD_abuse', \n",
    "            'Davidson_abuse', \n",
    "            # 'Founta_abuse',\n",
    "            'CAD_hate',\n",
    "            'Davidson_hate',\n",
    "            # 'Founta_hate'\n",
    "           ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e03de21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck perturbations with CAD_abuse.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0accf790524d4eb5847f84597208d3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck perturbations with Davidson_abuse.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce5ae45bc8d4f87a4aba7dafc16c09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck perturbations with CAD_hate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ff65ca5dc34bebb3df4dd73b29635b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck perturbations with Davidson_hate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1c40271d45455ca1d565b229669b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "necc_mask_preds = {}\n",
    "necc_mask_scores = {}\n",
    "suff_mask_preds = {}\n",
    "suff_mask_scores = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Classifying HateCheck perturbations with {}.\".format(dataset))\n",
    "  #  model = BertForSequenceClassification.from_pretrained(models_dir +'BERT_{}_weighted/Final'.format(dataset))\n",
    "    model = BertForSequenceClassification.from_pretrained(\"Models/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    \n",
    "    total_len = sum(len(nn) for nn in perts['necc_perturbed']) + sum(len(nn) for nn in perts['suff_perturbed'])\n",
    " \n",
    "    with tqdm(total=total_len) as pbar:\n",
    "            \n",
    "        necc_mask_preds[dataset] = []\n",
    "        necc_mask_scores[dataset] = []\n",
    "    \n",
    "        for tt in necc_masked:\n",
    "            pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "            necc_mask_preds[dataset].append(pp)\n",
    "            necc_mask_scores[dataset].append(ss)\n",
    "            \n",
    "        suff_mask_preds[dataset] = []\n",
    "        suff_mask_scores[dataset] = []\n",
    "    \n",
    "        for tt in suff_masked:\n",
    "            pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "            suff_mask_preds[dataset].append(pp)\n",
    "            suff_mask_scores[dataset].append(ss)\n",
    "            \n",
    "        \n",
    "final_results = {\n",
    "                'necc_mask_preds': necc_mask_preds,\n",
    "                'necc_mask_scores': necc_mask_scores,\n",
    "                'suff_mask_preds': suff_mask_preds,\n",
    "                'suff_mask_scores': suff_mask_scores\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a54879-14ce-4b70-b338-b9e674f043bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_results, open(\"Data/final_results.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a5ab1c-3726-4500-bdd5-755c41e10832",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pickle.load(open(\"Data/final_results.pickle\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c0a1331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck instances with CAD_abuse.\n",
      "Classifying HateCheck instances with Davidson_abuse.\n",
      "Classifying HateCheck instances with CAD_hate.\n",
      "Classifying HateCheck instances with Davidson_hate.\n"
     ]
    }
   ],
   "source": [
    "orig_preds = {}\n",
    "orig_scores = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Classifying HateCheck instances with {}.\".format(dataset))\n",
    "  #  model = BertForSequenceClassification.from_pretrained(models_dir +'BERT_{}_weighted/Final'.format(dataset))\n",
    "    model = BertForSequenceClassification.from_pretrained(\"Models/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "\n",
    "    orig_preds[dataset], orig_scores[dataset] = get_preds_and_scores(perts['orig_texts'], tokenizer, model, pbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51b8b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results[\"orig_preds\"] = orig_preds\n",
    "final_results[\"orig_scores\"] = orig_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1873966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_results, open(\"Data/final_results_masked_2.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdbe93bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline_preds': {'CAD_abuse': 0.0392,\n",
       "  'Davidson_abuse': 0.0854,\n",
       "  'CAD_hate': 0.0112,\n",
       "  'Davidson_hate': 0.039},\n",
       " 'baseline_scores': {'CAD_abuse': 0.04640193686255952,\n",
       "  'Davidson_abuse': 0.08730076259395574,\n",
       "  'CAD_hate': 0.01633027881102171,\n",
       "  'Davidson_hate': 0.05334486834288109}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_preds = pickle.load(open(\"Data/Classifier_baselines_2.pickle\", \"rb\"))\n",
    "baseline_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa876684",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "necc_results_mask = {}\n",
    "necc_results_mask_nb = {}\n",
    "suff_results_mask = {}\n",
    "suff_results_mask_nb = {}\n",
    "\n",
    "baselines = pickle.load(open(\"Data/Classifier_baselines_2.pickle\", \"rb\"))\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    ## NECCESSITY CALCULATIONS  \n",
    "    necc_mask = []\n",
    "    for oo, pp, mm in zip(final_results['orig_preds'][dataset], \n",
    "                          final_results['necc_mask_preds'][dataset], \n",
    "                          perts['necc_masks']):\n",
    "        pp = np.array(pp)\n",
    "        necc_mask.append(calc_necc(oo, pp, mm))\n",
    "    necc_results_mask[dataset] = necc_mask\n",
    "    \n",
    "    necc_mask_nb = []\n",
    "    for oo, pp, mm in zip(final_results['orig_scores'][dataset], \n",
    "                          final_results['necc_mask_scores'][dataset], \n",
    "                          perts['necc_masks']):\n",
    "        pp = np.array(pp)\n",
    "        necc_mask_nb.append(calc_necc(oo, pp, mm))\n",
    "    necc_results_mask_nb[dataset] = necc_mask_nb\n",
    "    \n",
    "    ## SUFFICIENCY CALCULATIONS\n",
    "    baseline_pred = baselines['baseline_preds'][dataset]\n",
    "    baseline_score = baselines['baseline_scores'][dataset]\n",
    "    \n",
    "    suff_mask = []\n",
    "    for pp, mm in zip(final_results['suff_mask_preds'][dataset], perts['suff_masks']):\n",
    "        pp = np.array(pp)\n",
    "        suff_mask.append(calc_suff(baseline_pred, pp, mm))\n",
    "    suff_results_mask[dataset] = suff_mask\n",
    "\n",
    "    suff_mask_nb = []\n",
    "    for pp, mm in zip(final_results['suff_mask_scores'][dataset], perts['suff_masks']):\n",
    "        pp = np.array(pp)\n",
    "        suff_mask_nb.append(calc_suff(baseline_score, pp, mm))\n",
    "    suff_results_mask_nb[dataset] = suff_mask_nb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91eea098",
   "metadata": {},
   "outputs": [],
   "source": [
    "hatecheck_necc_suff_masked = {\n",
    "    'necc_results': necc_results_mask, \n",
    "    'necc_results_nb' : necc_results_mask_nb,\n",
    "    'suff_results': suff_results_mask,\n",
    "    'suff_results_nb' : suff_results_mask_nb,\n",
    "}\n",
    "\n",
    "pickle.dump(hatecheck_necc_suff_masked, open('Data/HateCheck_necc_suff_results_masked_2.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9dc951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0d0d9-5e6c-48ca-ae0a-97091f6ba1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c70eaa-29fd-4706-baa5-dd39d8c769ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4b02f-e931-4a0e-b4b8-d924a112ae9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82309edc-7e98-4b37-8f90-13cd23869385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
