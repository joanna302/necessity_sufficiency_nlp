{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fb9d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from random import shuffle\n",
    "from perturbation_functions import get_preds_and_scores, calc_suff, calc_necc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e44157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['orig_texts', 'necc_perturbed', 'suff_perturbed', 'necc_masks', 'suff_masks'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perts = pickle.load(open(\"Data/HateCheck_necc_suff_perturbations_3.pickle\",\"rb\"))\n",
    "perts['orig_texts'] = [tt.strip(' \\n') for tt in perts['orig_texts']]\n",
    "perts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45b9c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# add special tokens for URLs, emojis and mentions (--> see pre-processing)\n",
    "special_tokens_dict = {'additional_special_tokens': ['[USER]','[EMOJI]','[URL]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "datasets = ['CAD_abuse', \n",
    "            'Davidson_abuse', \n",
    "            # 'Founta_abuse',\n",
    "            'CAD_hate',\n",
    "            'Davidson_hate',\n",
    "            # 'Founta_hate'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a000c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck perturbations with CAD_abuse.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d6b520eb3c4a99b31ff3c0408f8bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck perturbations with Davidson_abuse.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e9c5a07ef0478d97b51e79b7be83df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "orig_preds = {}\n",
    "orig_scores = {}\n",
    "necc_preds = {}\n",
    "necc_scores = {}\n",
    "suff_preds = {}\n",
    "suff_scores = {}\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Classifying HateCheck perturbations with {}.\".format(dataset))\n",
    "    model = BertForSequenceClassification.from_pretrained(\"Models/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    \n",
    "    total_len = len(perts['orig_texts']) + sum(len(nn) for nn in perts['necc_perturbed']) + sum(len(nn) for nn in perts['suff_perturbed'])\n",
    " \n",
    "    with tqdm(total=total_len) as pbar:\n",
    "        orig_preds[dataset], orig_scores[dataset] = get_preds_and_scores(perts['orig_texts'], tokenizer, model, pbar)\n",
    "        \n",
    "        necc_preds[dataset] = []\n",
    "        necc_scores[dataset] = []\n",
    "    \n",
    "        for tt in perts['necc_perturbed']:\n",
    "            pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "            necc_preds[dataset].append(pp)\n",
    "            necc_scores[dataset].append(ss)\n",
    "            \n",
    "        suff_preds[dataset] = []\n",
    "        suff_scores[dataset] = []\n",
    "    \n",
    "        for tt in perts['suff_perturbed']:\n",
    "            pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "            suff_preds[dataset].append(pp)\n",
    "            suff_scores[dataset].append(ss)\n",
    "            \n",
    "        \n",
    "final_results = {\n",
    "                'orig_preds': orig_preds,\n",
    "                'orig_scores': orig_scores,\n",
    "                'necc_preds': necc_preds,\n",
    "                'necc_scores': necc_scores,\n",
    "                'suff_preds': suff_preds,\n",
    "                'suff_scores': suff_scores,\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4837fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_results, open(\"Data/HateCheck_necc_suff_preds_2.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4dabe15-5afa-4a7d-98d7-efaf179dbef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pickle.load(open(\"Data/HateCheck_necc_suff_preds_2.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36b5706",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/ILM/compound_dataset/train.txt\", \"r\") as ff:\n",
    "    compound_dataset = ff.read().split(\"\\n\\n\\n\")\n",
    "compound_dataset = [tt.strip(\" :`.,\") for tt in compound_dataset]\n",
    "shuffle(compound_dataset)\n",
    "compound_dataset = compound_dataset[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae63d2b7-bdb0-4bdf-a450-608836c4ddbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e0a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['CAD_abuse', \n",
    "            'Davidson_abuse', \n",
    "            # 'Founta_abuse',\n",
    "            'CAD_hate',\n",
    "            'Davidson_hate',\n",
    "            # 'Founta_hate'\n",
    "           ]\n",
    "\n",
    "baseline_preds = {}\n",
    "baseline_scores = {}\n",
    "\n",
    "for dataset in datasets: \n",
    "    model = BertForSequenceClassification.from_pretrained(\"Models/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    preds, scores = get_preds_and_scores(compound_dataset, tokenizer, model)\n",
    "    baseline_preds[dataset] = sum(preds)/len(preds)\n",
    "    baseline_scores[dataset] = sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01e9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({'baseline_preds':baseline_preds, 'baseline_scores':baseline_scores}, open(\"Classifier_baselines_2.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e6a8b-9662-4387-adc6-d9746c7280e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7be4b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "necc_results = {}\n",
    "necc_results_nb = {}\n",
    "suff_results = {}\n",
    "suff_results_nb = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    ## NECCESSITY CALCULATIONS\n",
    "    neccs = []\n",
    "    for oo, pp, mm in zip(final_results['orig_preds'][dataset], \n",
    "                          final_results['necc_preds'][dataset], \n",
    "                          perts['necc_masks']):\n",
    "        pp = np.array(pp)\n",
    "        neccs.append(calc_necc(oo, pp, mm))\n",
    "    necc_results[dataset] = neccs \n",
    "    \n",
    "    neccs_nb = []\n",
    "    for oo, pp, mm in zip(final_results['orig_scores'][dataset], \n",
    "                          final_results['necc_scores'][dataset], \n",
    "                          perts['necc_masks']):\n",
    "        pp = np.array(pp)\n",
    "        neccs_nb.append(calc_necc(oo, pp, mm))\n",
    "    necc_results_nb[dataset] = neccs_nb\n",
    "    \n",
    "    ## SUFFICIENCY CALCULATIONS\n",
    "    baseline_pred = baseline_preds[dataset]\n",
    "    baseline_score = baseline_scores[dataset]\n",
    "    \n",
    "    suffs = []\n",
    "    for pp, mm in zip(final_results['suff_preds'][dataset], perts['suff_masks']):\n",
    "        pp = np.array(pp)\n",
    "        suffs.append(calc_suff(baseline_pred, pp, mm))\n",
    "    suff_results[dataset] = suffs \n",
    "    \n",
    "    suffs_nb = []\n",
    "    for pp, mm in zip(final_results['suff_scores'][dataset], perts['suff_masks']):\n",
    "        pp = np.array(pp)\n",
    "        suffs_nb.append(calc_suff(baseline_score, pp, mm))\n",
    "    suff_results_nb[dataset] = suffs_nb     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23175599",
   "metadata": {},
   "outputs": [],
   "source": [
    "hatecheck_necc_suff_results = {\n",
    "    'necc_results': necc_results,\n",
    "    'necc_results_nb': necc_results_nb,\n",
    "    'suff_results': suff_results, \n",
    "    'suff_results_nb': suff_results_nb\n",
    "}\n",
    "\n",
    "pickle.dump(hatecheck_necc_suff_results, open('Data/HateCheck_necc_suff_results_all_2.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a850153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the predictions for all models for the entire hatecheck suite\n",
    "hc_test_cases_all = pd.read_csv(\"hatecheck-data/test_suite_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03cd9a56-594e-4a78-a6ec-c4664ac494d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0      functionality  case_id  \\\n",
      "0              0  derog_neg_emote_h        1   \n",
      "1              1  derog_neg_emote_h        2   \n",
      "2              2  derog_neg_emote_h        3   \n",
      "3              3  derog_neg_emote_h        4   \n",
      "4              4  derog_neg_emote_h        5   \n",
      "...          ...                ...      ...   \n",
      "3723        3723       spell_leet_h     3897   \n",
      "3724        3724       spell_leet_h     3898   \n",
      "3725        3725       spell_leet_h     3899   \n",
      "3726        3726       spell_leet_h     3900   \n",
      "3727        3727       spell_leet_h     3901   \n",
      "\n",
      "                                        test_case label_gold     target_ident  \\\n",
      "0                                  I hate women.     hateful            women   \n",
      "1                           I hate trans people.     hateful     trans people   \n",
      "2                             I hate gay people.     hateful       gay people   \n",
      "3                           I hate black people.     hateful     black people   \n",
      "4                        I hate disabled people.     hateful  disabled people   \n",
      "...                                           ...        ...              ...   \n",
      "3723       Gay people are all complete 4ssholes.     hateful       gay people   \n",
      "3724     Black people are all complete 4ssholes.     hateful     black people   \n",
      "3725  Disabled people are all complete 4ssholes.     hateful  disabled people   \n",
      "3726          Muslims are all complete 4ssholes.     hateful          Muslims   \n",
      "3727       Immigrants are all complete 4ssholes.     hateful       immigrants   \n",
      "\n",
      "     direction focus_words focus_lemma  ref_case_id  ref_templ_id  templ_id  \\\n",
      "0      general        hate        hate          NaN           NaN         1   \n",
      "1      general        hate        hate          NaN           NaN         1   \n",
      "2      general        hate        hate          NaN           NaN         1   \n",
      "3      general        hate        hate          NaN           NaN         1   \n",
      "4      general        hate        hate          NaN           NaN         1   \n",
      "...        ...         ...         ...          ...           ...       ...   \n",
      "3723   general    4ssholes     asshole       1232.0         256.0       866   \n",
      "3724   general    4ssholes     asshole       1233.0         256.0       866   \n",
      "3725   general    4ssholes     asshole       1234.0         256.0       866   \n",
      "3726   general    4ssholes     asshole       1235.0         256.0       866   \n",
      "3727   general    4ssholes     asshole       1236.0         256.0       866   \n",
      "\n",
      "                                   case_templ  \n",
      "0                        I hate [IDENTITY_P].  \n",
      "1                        I hate [IDENTITY_P].  \n",
      "2                        I hate [IDENTITY_P].  \n",
      "3                        I hate [IDENTITY_P].  \n",
      "4                        I hate [IDENTITY_P].  \n",
      "...                                       ...  \n",
      "3723  [IDENTITY_P] are all complete 4ssholes.  \n",
      "3724  [IDENTITY_P] are all complete 4ssholes.  \n",
      "3725  [IDENTITY_P] are all complete 4ssholes.  \n",
      "3726  [IDENTITY_P] are all complete 4ssholes.  \n",
      "3727  [IDENTITY_P] are all complete 4ssholes.  \n",
      "\n",
      "[3728 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "print(hc_test_cases_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "577d8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_test_cases_all_l = hc_test_cases_all.test_case.tolist()\n",
    "hc_preds = {}\n",
    "hc_scores = {}\n",
    "for dataset in datasets: \n",
    "    model = BertForSequenceClassification.from_pretrained(\"Models/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    preds, scores = get_preds_and_scores(hc_test_cases_all_l, tokenizer, model)\n",
    "    hc_preds[dataset] = preds\n",
    "    hc_scores[dataset] = scores\n",
    "\n",
    "pickle.dump({'preds': hc_preds, 'scores':hc_scores}, open('Data/HateCheck_results_all_models_2.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d19e934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    hc_test_cases_all['{}_pred'.format(dataset)] = hc_preds[dataset]\n",
    "    hc_test_cases_all['{}_score'.format(dataset)] = hc_scores[dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31924981",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(hc_test_cases_all, open('Data/HateCheck_templates_and_results_2.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357da42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
