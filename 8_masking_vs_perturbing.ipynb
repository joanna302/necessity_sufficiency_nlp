{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec7fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from random import shuffle\n",
    "from perturbation_functions import get_preds_and_scores, calc_suff, calc_necc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b59671b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['orig_texts', 'necc_perturbed', 'suff_perturbed', 'necc_masks', 'suff_masks'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perts = pickle.load(open(\"Data/HateCheck_necc_suff_perturbations.pickle\",\"rb\"))\n",
    "perts['orig_texts'] = [tt.strip(' \\n') for tt in perts['orig_texts']]\n",
    "perts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd0780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "necc_masked = []\n",
    "for orig_text, necc_mask in zip(perts['orig_texts'], perts['necc_masks']):\n",
    "    orig_text = orig_text.strip().split()\n",
    "    masked = []\n",
    "    for masks in necc_mask:\n",
    "        masked.append(\" \".join(['[MASK]' if mm else tt for tt, mm in zip(orig_text, masks)]))\n",
    "    necc_masked.append(masked)\n",
    "    \n",
    "suff_masked = [] \n",
    "for orig_text, suff_mask in zip(perts['orig_texts'], perts['suff_masks']):\n",
    "    orig_text = orig_text.strip().split()\n",
    "    masked = []\n",
    "    for masks in suff_mask:\n",
    "        masked.append(\" \".join(['[MASK]' if mm else tt for tt, mm in zip(orig_text, masks)]))\n",
    "    suff_masked.append(masked)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4207f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# add special tokens for URLs, emojis and mentions (--> see pre-processing)\n",
    "special_tokens_dict = {'additional_special_tokens': ['[USER]','[EMOJI]','[URL]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "#datasets = ['CAD_abuse', \n",
    "#            'Davidson_abuse', \n",
    "#            'Founta_abuse',\n",
    "#            'CAD_hate',\n",
    "#            'Davidson_hate',\n",
    "#            'Founta_hate']\n",
    "\n",
    "\n",
    "datasets = ['CAD_abuse', \n",
    "            'Davidson_abuse', \n",
    "            'CAD_hate',\n",
    "            'Davidson_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03de21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck perturbations with CAD_abuse.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f62db578254b3caa988495323ef278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck perturbations with Davidson_abuse.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63f637e4953421fa67d7aefc717ce02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "necc_mask_preds = {}\n",
    "necc_mask_scores = {}\n",
    "suff_mask_preds = {}\n",
    "suff_mask_scores = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Classifying HateCheck perturbations with {}.\".format(dataset))\n",
    "  #  model = BertForSequenceClassification.from_pretrained(models_dir +'BERT_{}_weighted/Final'.format(dataset))\n",
    "    model = BertForSequenceClassification.from_pretrained(\"Model/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    \n",
    "    total_len = sum(len(nn) for nn in perts['necc_perturbed']) + sum(len(nn) for nn in perts['suff_perturbed'])\n",
    " \n",
    "    with tqdm(total=total_len) as pbar:\n",
    "            \n",
    "        necc_mask_preds[dataset] = []\n",
    "        necc_mask_scores[dataset] = []\n",
    "    \n",
    "        for tt in necc_masked:\n",
    "            pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "            necc_mask_preds[dataset].append(pp)\n",
    "            necc_mask_scores[dataset].append(ss)\n",
    "            \n",
    "        suff_mask_preds[dataset] = []\n",
    "        suff_mask_scores[dataset] = []\n",
    "    \n",
    "        for tt in suff_masked:\n",
    "            pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "            suff_mask_preds[dataset].append(pp)\n",
    "            suff_mask_scores[dataset].append(ss)\n",
    "            \n",
    "        \n",
    "final_results = {\n",
    "                'necc_mask_preds': necc_mask_preds,\n",
    "                'necc_mask_scores': necc_mask_scores,\n",
    "                'suff_mask_preds': suff_mask_preds,\n",
    "                'suff_mask_scores': suff_mask_scores\n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0a1331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck instances with CAD_abuse.\n",
      "Classifying HateCheck instances with Davidson_abuse.\n",
      "Classifying HateCheck instances with Founta_abuse.\n",
      "Classifying HateCheck instances with CAD_hate.\n",
      "Classifying HateCheck instances with Davidson_hate.\n",
      "Classifying HateCheck instances with Founta_hate.\n"
     ]
    }
   ],
   "source": [
    "orig_preds = {}\n",
    "orig_scores = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Classifying HateCheck instances with {}.\".format(dataset))\n",
    "  #  model = BertForSequenceClassification.from_pretrained(models_dir +'BERT_{}_weighted/Final'.format(dataset))\n",
    "    model = BertForSequenceClassification.from_pretrained(\"Model{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "\n",
    "    orig_preds[dataset], orig_scores[dataset] = get_preds_and_scores(perts['orig_texts'], tokenizer, model, pbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b8b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results[\"orig_preds\"] = orig_preds\n",
    "final_results[\"orig_scores\"] = orig_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1873966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_results, open(\"Data/final_results_masked.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe93bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline_preds': {'CAD_abuse': 0.0386,\n",
       "  'Davidson_abuse': 0.0728,\n",
       "  'Founta_hate': 0.0648,\n",
       "  'CAD_hate': 0.0252,\n",
       "  'Davidson_hate': 0.0238,\n",
       "  'Founta_abuse': 0.0202},\n",
       " 'baseline_scores': {'CAD_abuse': 0.04610298428169917,\n",
       "  'Davidson_abuse': 0.07315641217394732,\n",
       "  'Founta_hate': 0.07248694326588884,\n",
       "  'CAD_hate': 0.03189067478131037,\n",
       "  'Davidson_hate': 0.03256542438273318,\n",
       "  'Founta_abuse': 0.031436307859700176}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_preds = pickle.load(open(\"Data/Classifier_baselines.pickle\", \"rb\"))\n",
    "baseline_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa876684",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "necc_results_mask = {}\n",
    "necc_results_mask_nb = {}\n",
    "suff_results_mask = {}\n",
    "suff_results_mask_nb = {}\n",
    "\n",
    "baselines = pickle.load(open(\"Data/Classifier_baselines.pickle\", \"rb\"))\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    ## NECCESSITY CALCULATIONS  \n",
    "    necc_mask = []\n",
    "    for oo, pp, mm in zip(final_results['orig_preds'][dataset], \n",
    "                          final_results['necc_mask_preds'][dataset], \n",
    "                          perts['necc_masks']):\n",
    "        pp = np.array(pp)\n",
    "        necc_mask.append(calc_necc(oo, pp, mm))\n",
    "    necc_results_mask[dataset] = necc_mask\n",
    "    \n",
    "    necc_mask_nb = []\n",
    "    for oo, pp, mm in zip(final_results['orig_scores'][dataset], \n",
    "                          final_results['necc_mask_scores'][dataset], \n",
    "                          perts['necc_masks']):\n",
    "        pp = np.array(pp)\n",
    "        necc_mask_nb.append(calc_necc(oo, pp, mm))\n",
    "    necc_results_mask_nb[dataset] = necc_mask_nb\n",
    "    \n",
    "    ## SUFFICIENCY CALCULATIONS\n",
    "    baseline_pred = baselines['baseline_preds'][dataset]\n",
    "    baseline_score = baselines['baseline_scores'][dataset]\n",
    "    \n",
    "    suff_mask = []\n",
    "    for pp, mm in zip(final_results['suff_mask_preds'][dataset], perts['suff_masks']):\n",
    "        pp = np.array(pp)\n",
    "        suff_mask.append(calc_suff(baseline_pred, pp, mm))\n",
    "    suff_results_mask[dataset] = suff_mask\n",
    "\n",
    "    suff_mask_nb = []\n",
    "    for pp, mm in zip(final_results['suff_mask_scores'][dataset], perts['suff_masks']):\n",
    "        pp = np.array(pp)\n",
    "        suff_mask_nb.append(calc_suff(baseline_score, pp, mm))\n",
    "    suff_results_mask_nb[dataset] = suff_mask_nb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eea098",
   "metadata": {},
   "outputs": [],
   "source": [
    "hatecheck_necc_suff_masked = {\n",
    "    'necc_results': necc_results_mask, \n",
    "    'necc_results_nb' : necc_results_mask_nb,\n",
    "    'suff_results': suff_results_mask,\n",
    "    'suff_results_nb' : suff_results_mask_nb,\n",
    "}\n",
    "\n",
    "pickle.dump(hatecheck_necc_suff_masked, open('Data/HateCheck_necc_suff_results_masked.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9dc951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
