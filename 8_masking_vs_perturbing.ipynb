{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec7fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from random import shuffle\n",
    "from perturbation_functions import get_preds_and_scores, calc_suff, calc_necc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59671b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['orig_texts', 'necc_perturbed', 'suff_perturbed', 'necc_masks', 'suff_masks'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perts = pickle.load(open(\"Data/intermediate outputs/HateCheck_necc_suff_perturbations.pickle\",\"rb\"))\n",
    "perts['orig_texts'] = [tt.strip(' \\n') for tt in perts['orig_texts']]\n",
    "perts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd0780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "necc_masked = []\n",
    "for orig_text, necc_mask in zip(perts['orig_texts'], perts['necc_masks']):\n",
    "    orig_text = orig_text.strip().split()\n",
    "    masked = []\n",
    "    for masks in necc_mask:\n",
    "        masked.append(\" \".join(['[MASK]' if mm else tt for tt, mm in zip(orig_text, masks)]))\n",
    "    necc_masked.append(masked)\n",
    "    \n",
    "suff_masked = [] \n",
    "for orig_text, suff_mask in zip(perts['orig_texts'], perts['suff_masks']):\n",
    "    orig_text = orig_text.strip().split()\n",
    "    masked = []\n",
    "    for masks in suff_mask:\n",
    "        masked.append(\" \".join(['[MASK]' if mm else tt for tt, mm in zip(orig_text, masks)]))\n",
    "    suff_masked.append(masked)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b4207f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# add special tokens for URLs, emojis and mentions (--> see pre-processing)\n",
    "special_tokens_dict = {'additional_special_tokens': ['[USER]','[EMOJI]','[URL]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "datasets = ['CAD_abuse', \n",
    "            'Davidson_abuse', \n",
    "            'Founta_abuse',\n",
    "            'CAD_hate',\n",
    "            'Davidson_hate',\n",
    "            'Founta_hate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e03de21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck perturbations with CAD_abuse.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: 'Models/Classifiers/CAD_abuse'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/mva/S2/NLP/necessity-sufficiency/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/mva/S2/NLP/necessity-sufficiency/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/mva/S2/NLP/necessity-sufficiency/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'Models/Classifiers/CAD_abuse'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifying HateCheck perturbations with \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dataset))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#  model = BertForSequenceClassification.from_pretrained(models_dir +'BERT_{}_weighted/Final'.format(dataset))\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m   model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModels/Classifiers/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m   model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[1;32m     11\u001b[0m   model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Documents/mva/S2/NLP/necessity-sufficiency/.venv/lib/python3.9/site-packages/transformers/modeling_utils.py:3464\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   3463\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m-> 3464\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3465\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3466\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3467\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3468\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3469\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3470\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3471\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3472\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3473\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3474\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3475\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3476\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3477\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3478\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3479\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   3480\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/mva/S2/NLP/necessity-sufficiency/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:469\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: 'Models/Classifiers/CAD_abuse'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "\n",
    "necc_mask_preds = {}\n",
    "necc_mask_scores = {}\n",
    "suff_mask_preds = {}\n",
    "suff_mask_scores = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Classifying HateCheck perturbations with {}.\".format(dataset))\n",
    "  #  model = BertForSequenceClassification.from_pretrained(models_dir +'BERT_{}_weighted/Final'.format(dataset))\n",
    "    model = BertForSequenceClassification.from_pretrained(\"Models/Classifiers/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    \n",
    "    total_len = sum(len(nn) for nn in perts['necc_perturbed']) + sum(len(nn) for nn in perts['suff_perturbed'])\n",
    " \n",
    "    with tqdm(total=total_len) as pbar:\n",
    "            \n",
    "        necc_mask_preds[dataset] = []\n",
    "        necc_mask_scores[dataset] = []\n",
    "    \n",
    "        for tt in necc_masked:\n",
    "            pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "            necc_mask_preds[dataset].append(pp)\n",
    "            necc_mask_scores[dataset].append(ss)\n",
    "            \n",
    "        suff_mask_preds[dataset] = []\n",
    "        suff_mask_scores[dataset] = []\n",
    "    \n",
    "        for tt in suff_masked:\n",
    "            pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "            suff_mask_preds[dataset].append(pp)\n",
    "            suff_mask_scores[dataset].append(ss)\n",
    "            \n",
    "        \n",
    "final_results = {\n",
    "                'necc_mask_preds': necc_mask_preds,\n",
    "                'necc_mask_scores': necc_mask_scores,\n",
    "                'suff_mask_preds': suff_mask_preds,\n",
    "                'suff_mask_scores': suff_mask_scores\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c0a1331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck instances with CAD_abuse.\n",
      "Classifying HateCheck instances with Davidson_abuse.\n",
      "Classifying HateCheck instances with Founta_abuse.\n",
      "Classifying HateCheck instances with CAD_hate.\n",
      "Classifying HateCheck instances with Davidson_hate.\n",
      "Classifying HateCheck instances with Founta_hate.\n"
     ]
    }
   ],
   "source": [
    "orig_preds = {}\n",
    "orig_scores = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Classifying HateCheck instances with {}.\".format(dataset))\n",
    "  #  model = BertForSequenceClassification.from_pretrained(models_dir +'BERT_{}_weighted/Final'.format(dataset))\n",
    "    model = BertForSequenceClassification.from_pretrained(\"Models/Classifiers/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "\n",
    "    orig_preds[dataset], orig_scores[dataset] = get_preds_and_scores(perts['orig_texts'], tokenizer, model, pbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51b8b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results[\"orig_preds\"] = orig_preds\n",
    "final_results[\"orig_scores\"] = orig_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1873966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_results, open(\"Data/final_results_masked.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdbe93bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline_preds': {'CAD_abuse': 0.0386,\n",
       "  'Davidson_abuse': 0.0728,\n",
       "  'Founta_hate': 0.0648,\n",
       "  'CAD_hate': 0.0252,\n",
       "  'Davidson_hate': 0.0238,\n",
       "  'Founta_abuse': 0.0202},\n",
       " 'baseline_scores': {'CAD_abuse': 0.04610298428169917,\n",
       "  'Davidson_abuse': 0.07315641217394732,\n",
       "  'Founta_hate': 0.07248694326588884,\n",
       "  'CAD_hate': 0.03189067478131037,\n",
       "  'Davidson_hate': 0.03256542438273318,\n",
       "  'Founta_abuse': 0.031436307859700176}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_preds = pickle.load(open(\"Data/Classifier_baselines.pickle\", \"rb\"))\n",
    "baseline_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa876684",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "necc_results_mask = {}\n",
    "necc_results_mask_nb = {}\n",
    "suff_results_mask = {}\n",
    "suff_results_mask_nb = {}\n",
    "\n",
    "baselines = pickle.load(open(\"Data/Classifier_baselines.pickle\", \"rb\"))\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    ## NECCESSITY CALCULATIONS  \n",
    "    necc_mask = []\n",
    "    for oo, pp, mm in zip(final_results['orig_preds'][dataset], \n",
    "                          final_results['necc_mask_preds'][dataset], \n",
    "                          perts['necc_masks']):\n",
    "        pp = np.array(pp)\n",
    "        necc_mask.append(calc_necc(oo, pp, mm))\n",
    "    necc_results_mask[dataset] = necc_mask\n",
    "    \n",
    "    necc_mask_nb = []\n",
    "    for oo, pp, mm in zip(final_results['orig_scores'][dataset], \n",
    "                          final_results['necc_mask_scores'][dataset], \n",
    "                          perts['necc_masks']):\n",
    "        pp = np.array(pp)\n",
    "        necc_mask_nb.append(calc_necc(oo, pp, mm))\n",
    "    necc_results_mask_nb[dataset] = necc_mask_nb\n",
    "    \n",
    "    ## SUFFICIENCY CALCULATIONS\n",
    "    baseline_pred = baselines['baseline_preds'][dataset]\n",
    "    baseline_score = baselines['baseline_scores'][dataset]\n",
    "    \n",
    "    suff_mask = []\n",
    "    for pp, mm in zip(final_results['suff_mask_preds'][dataset], perts['suff_masks']):\n",
    "        pp = np.array(pp)\n",
    "        suff_mask.append(calc_suff(baseline_pred, pp, mm))\n",
    "    suff_results_mask[dataset] = suff_mask\n",
    "\n",
    "    suff_mask_nb = []\n",
    "    for pp, mm in zip(final_results['suff_mask_scores'][dataset], perts['suff_masks']):\n",
    "        pp = np.array(pp)\n",
    "        suff_mask_nb.append(calc_suff(baseline_score, pp, mm))\n",
    "    suff_results_mask_nb[dataset] = suff_mask_nb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91eea098",
   "metadata": {},
   "outputs": [],
   "source": [
    "hatecheck_necc_suff_masked = {\n",
    "    'necc_results': necc_results_mask, \n",
    "    'necc_results_nb' : necc_results_mask_nb,\n",
    "    'suff_results': suff_results_mask,\n",
    "    'suff_results_nb' : suff_results_mask_nb,\n",
    "}\n",
    "\n",
    "pickle.dump(hatecheck_necc_suff_masked, open('Data/HateCheck_necc_suff_results_masked.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9dc951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
